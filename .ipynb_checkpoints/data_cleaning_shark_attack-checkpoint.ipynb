{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shark Attack Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'find_stack_level' from 'pandas.util._exceptions' (C:\\Users\\lvgui\\anaconda3\\lib\\site-packages\\pandas\\util\\_exceptions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18616/171194120.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Data wrangling and cleaning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig_init\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m from pandas.core.api import (\n\u001b[0m\u001b[0;32m     52\u001b[0m     \u001b[1;31m# dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mInt8Dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\api.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0misnull\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnotna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnotnull\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithms\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfactorize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_counts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrays\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboolean\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBooleanDtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstruction\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvalidate_indices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m from pandas.core.indexers.utils import (\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mcheck_array_indexer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mcheck_key_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcheck_setitem_lengths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdeprecate_ndim_indexing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexers\\utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_typing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAnyArrayLike\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfind_stack_level\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m from pandas.core.dtypes.common import (\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'find_stack_level' from 'pandas.util._exceptions' (C:\\Users\\lvgui\\anaconda3\\lib\\site-packages\\pandas\\util\\_exceptions.py)"
     ]
    }
   ],
   "source": [
    "# Data wrangling and cleaning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Data visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# To handle species.txt file accessible through a url\n",
    "import urllib\n",
    "\n",
    "# To create \"year\", \"month\", and \"day\" columns\n",
    "from datetime import datetime\n",
    "\n",
    "# Setting preferences for pandas\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading file into a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/attacks.csv\", \"rb\") as file:\n",
    "    file_lines = file.readlines(500)\n",
    "    for line in file_lines:\n",
    "        print(line.decode('latin-1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shark = pd.read_csv(\"data/attacks.csv\", sep=\",\", encoding=\"latin-1\")\n",
    "\n",
    "print(f\"DataFrame shape = {shark.shape}\\n\")\n",
    "print(f\"{shark.info()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shark.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardizing column names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_shark_cols = [col.strip().lower().replace(\" \", \"_\") for col in shark.columns]\n",
    "shark.columns = standardized_shark_cols\n",
    "\n",
    "shark.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addressing null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 6))\n",
    "\n",
    "null_count_cols = shark.isna().sum().sort_values()\n",
    "\n",
    "plt.bar(x=null_count_cols.index, height=shark.shape[0], color=\"blue\", label=\"Non-null\")\n",
    "plt.bar(x=null_count_cols.index, height=null_count_cols, bottom=shark.shape[0]-null_count_cols, color=\"red\", label=\"Null\")\n",
    "\n",
    "# Line-drawing and text annotation\n",
    "plt.axhline(y=shark.shape[0]/2, linestyle=\"--\")\n",
    "ax.annotate(\"50%\", xy=(23.5, shark.shape[0]/2 + 300), fontsize=25)\n",
    "\n",
    "# x-labels\n",
    "ax.set_xlabel(\"Column Name\", fontsize=14)\n",
    "plt.xticks(rotation=90, fontsize=12)\n",
    "\n",
    "# y-labels\n",
    "ax.set_ylabel(\"Null Value Count\", fontsize=14)\n",
    "plt.yticks(fontsize=13)\n",
    "\n",
    "# Title and legend\n",
    "ax.set_title(\"Null Values per Column\", fontsize=18)\n",
    "plt.legend(fontsize=16, loc='upper left', bbox_to_anchor=(1, 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shark[\"null_count\"] = shark.isna().sum(axis=1)\n",
    "shark[\"null_count\"].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shark.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (20, 9))\n",
    "\n",
    "# Obtaining value_counts from shark DataFrame\n",
    "null_percentages = shark[\"null_count\"].value_counts(normalize=True) * 100\n",
    "\n",
    "# Creating list comprehension for color scheme\n",
    "color = [\"darkred\" if (nan_count >= 12) else \"lightcoral\" for nan_count in null_percentages.index]\n",
    "\n",
    "plt.barh(y=null_percentages.index, width=null_percentages, color=color)\n",
    "\n",
    "# Line-drawing and text annotation\n",
    "plt.axhline(y=12, linestyle=\"--\")\n",
    "ax.annotate(\"50%\", xy=(66, 12.5), fontsize=25)\n",
    "\n",
    "# x-labels\n",
    "ax.set_xlabel(\"Row count [%]\", fontsize=16)\n",
    "plt.xticks(fontsize=15)\n",
    "\n",
    "# y-labels\n",
    "ax.set_ylabel(\"Null Count\", fontsize=16)\n",
    "plt.yticks(ticks=[3, 6, 9, 12, 15, 18, 21, 24], fontsize=15)\n",
    "\n",
    "# Title\n",
    "ax.set_title(\"Null Values per Row\", fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "too_many_nan_mask = shark[shark[\"null_count\"] >=20]\n",
    "shark_clean = shark.drop(too_many_nan_mask.index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 6))\n",
    "\n",
    "clean_null_count_cols = shark_clean.isnull().sum().sort_values()\n",
    "\n",
    "plt.bar(x=clean_null_count_cols.index, height=shark_clean.shape[0], color=\"blue\", label=\"Non-null\")\n",
    "plt.bar(x=clean_null_count_cols.index, height=clean_null_count_cols, bottom=shark_clean.shape[0]-clean_null_count_cols, color=\"red\", label=\"Null\")\n",
    "\n",
    "# Line-drawing and text annotation\n",
    "plt.axhline(y=shark_clean.shape[0]/2, linestyle=\"--\")\n",
    "ax.annotate(\"50%\", xy=(25.5, shark_clean.shape[0]/2 + 100), fontsize=25)\n",
    "\n",
    "# x-labels\n",
    "ax.set_xlabel(\"Column Name\", fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=13)\n",
    "\n",
    "# y-labels\n",
    "ax.set_ylabel(\"Null Value Count\", fontsize=14)\n",
    "plt.yticks(fontsize=13)\n",
    "\n",
    "# Title and legend\n",
    "ax.set_title(\"Null Values per Column - Cleaner Dataset\", fontsize=18)\n",
    "plt.legend(fontsize=16, loc='upper left', bbox_to_anchor=(1, 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of unique values in column 'unnamed:_22': {shark_clean['unnamed:_22'].nunique()}\")\n",
    "print(list(shark_clean[\"unnamed:_22\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of unique values in column 'unnamed:_23': {shark_clean['unnamed:_23'].nunique()}\")\n",
    "print(list(shark_clean[\"unnamed:_23\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shark_clean = shark_clean.drop(columns=[\"unnamed:_22\", \"unnamed:_23\"])\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (15, 6))\n",
    "\n",
    "clean_null_count_cols = shark_clean.isnull().sum().sort_values()\n",
    "\n",
    "plt.bar(x=clean_null_count_cols.index, height=shark_clean.shape[0], color=\"blue\", label=\"Non-null\")\n",
    "plt.bar(x=clean_null_count_cols.index, height=clean_null_count_cols, bottom=shark_clean.shape[0]-clean_null_count_cols, color=\"red\", label=\"Null\")\n",
    "\n",
    "# Line-drawing and text annotation\n",
    "plt.axhline(y=shark_clean.shape[0]/2, linestyle=\"--\")\n",
    "ax.annotate(\"50%\", xy=(22, shark_clean.shape[0]/2 + 100), fontsize=25)\n",
    "\n",
    "# x-labels\n",
    "ax.set_xlabel(\"Column Name\", fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=13)\n",
    "\n",
    "# y-labels\n",
    "ax.set_ylabel(\"Null Value Count\", fontsize=14)\n",
    "plt.yticks(fontsize=13)\n",
    "\n",
    "# Title and legend\n",
    "ax.set_title(\"Null Values per Column - Cleaner Dataset\", fontsize=18)\n",
    "plt.legend(fontsize=16, loc='upper left', bbox_to_anchor=(1, 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treating specific columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"activity\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shark_clean[\"activity\"].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_regex = r\"swimming|div|fish|surf|bath|boat|wading|kayak|snorkel|sail|compet|boogie board\"\n",
    "\n",
    "shark_clean[\"clean_activity\"] = shark_clean[\"activity\"].fillna(\"\").apply(\n",
    "    lambda act:\n",
    "    re.findall(activity_regex, act.lower())[0] if len(re.findall(activity_regex, act.lower())) > 0\n",
    "    else \"other\"\n",
    ")\n",
    "shark_clean[[\"activity\",\"clean_activity\"]].sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename values to improve readability\n",
    "\n",
    "# Create dict with new names\n",
    "act_name = {\n",
    "    \"div\": \"diving\",\n",
    "    \"surf\": \"surfing\",\n",
    "    \"fish\": \"fishing\",\n",
    "    \"bath\": \"bathing\",\n",
    "    \"kayak\": \"kayaking\",\n",
    "    \"snorkel\": \"snorkeling\",\n",
    "    \"sail\": \"sailing\",\n",
    "    \"compet\": \"competing\",\n",
    "    \"boogie board\": \"boogie boarding\"\n",
    "}\n",
    "\n",
    "# Apply to \"clean_activity\" column\n",
    "shark_clean[\"clean_activity\"] = shark_clean[\"clean_activity\"].apply(\n",
    "    lambda act: act_name[act]\n",
    "    if act in act_name.keys()\n",
    "    else act\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shark_clean[\"clean_activity\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"age\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_age = (\n",
    "    shark_clean[\"age\"]\n",
    "    .dropna()\n",
    "    .apply(lambda age: int(re.findall(r\"[0-9]+\", age)[0])\n",
    "            if len(re.findall(r\"[0-9]+\", age)) > 0\n",
    "            else 0\n",
    "           )\n",
    "    .astype(int)\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "mean_age = int(mean_age)\n",
    "mean_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shark_clean[\"clean_age\"] = (\n",
    "    shark_clean[\"age\"]\n",
    "    .fillna(str(mean_age))\n",
    "    .apply(\n",
    "        lambda age: int(re.findall(r\"[0-9]+\", age)[0])\n",
    "        if len(re.findall(r\"[0-9]+\", age)) > 0\n",
    "        else str(mean_age)\n",
    "           )\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "shark_clean[[\"age\", \"clean_age\"]].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"case_number\", \"case_number.1\" and \"case_number.2\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are \"case_number.1\" and \"case_number.2\" exact copies of \"case_number\"?\n",
    "case_number_eq_check = shark_clean[[\"case_number\", \"case_number.1\", \"case_number.2\"]].apply(\n",
    "    lambda row: all(row[col] == row[\"case_number\"] for col in [\"case_number.1\", \"case_number.2\"]), axis=1\n",
    ")\n",
    "case_number_eq_check.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"'case_number', 'case_number.1' and 'case_number.2' diverge in {shark_clean.shape[0] - case_number_eq_check.sum()} rows\")\n",
    "\n",
    "shark_clean[case_number_eq_check == False][[\"case_number\", \"case_number.1\", \"case_number.2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For some reason, \"case_number.1\" and \"case_number.2\" are not exact copies of \"case_number\",\n",
    "# diverging in 31 rows.\n",
    "# I have decided to drop two of these columns, the criteria for this selection being the two columns\n",
    "# with the highest number of null values.\n",
    "\n",
    "shark_clean[[\"case_number\", \"case_number.1\", \"case_number.2\"]].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shark_clean = shark_clean.drop(columns=[\"case_number.1\", \"case_number.2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the one null value in \"case_number\"\n",
    "\n",
    "shark_clean = shark_clean.drop(index=shark_clean[shark_clean[\"case_number\"].isna()].index)\n",
    "shark_clean[\"case_number\"].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"date\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_date(date_str):\n",
    "    m_dict = {\n",
    "        \"jan\": \"01\",\n",
    "        \"feb\": \"02\",\n",
    "        \"mar\": \"03\",\n",
    "        \"apr\": \"04\",\n",
    "        \"may\": \"05\",\n",
    "        \"jun\": \"06\",\n",
    "        \"jul\": \"07\",\n",
    "        \"aug\": \"08\",\n",
    "        \"sep\": \"09\",\n",
    "        \"oct\": \"10\",\n",
    "        \"nov\": \"11\",\n",
    "        \"dec\": \"12\"\n",
    "    }\n",
    "\n",
    "    pattern = r'(\\d{2})\\-([a-z]{3})\\-(\\d{4})'\n",
    "    match = re.search(pattern, date_str.lower())\n",
    "\n",
    "    if match:\n",
    "        year = match.group(3)\n",
    "        month = m_dict[match.group(2)] if match.group(2) in m_dict.keys() else \"01\"\n",
    "        day = match.group(1) if int(match.group(1)) <= 31 else \"01\"\n",
    "        return f\"{year}-{month.zfill(2)}-{day.zfill(2)}\"\n",
    "\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def parse_date(date_str):\n",
    "    standardized_date = standardize_date(date_str)\n",
    "\n",
    "    if standardized_date:\n",
    "        try:\n",
    "            date = datetime.strptime(standardized_date, '%Y-%m-%d')\n",
    "            year = date.year if date.year < 2023 else 1900\n",
    "            month = date.month if 1 <= date.month <= 12 else 1\n",
    "            day = date.day if 1 <= date.day <= 31 else 1\n",
    "            return year, month, day\n",
    "\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    return 1900, 1, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new DataFrame with the extracted year, month, and day\n",
    "shark_clean[[\"clean_year\", \"clean_month\", \"clean_day\"]] = pd.DataFrame(shark_clean['date'].apply(parse_date).tolist(),\n",
    "                        columns=['year', 'month', 'day'])\n",
    "\n",
    "shark_clean[[\"date\", \"clean_year\", \"clean_month\", \"clean_day\"]].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shark_clean[[\"clean_year\", \"clean_month\", \"clean_day\"]].fillna({\"clean_day\": \"01\", \"clean_month\": \"01\", \"clean_year\": \"1900\"}).isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values and cast column values as int\n",
    "shark_clean[[\"clean_year\", \"clean_month\", \"clean_day\"]] = (\n",
    "    shark_clean[[\"clean_year\", \"clean_month\", \"clean_day\"]]\n",
    "    .fillna({\"clean_day\": \"01\", \"clean_month\": \"01\", \"clean_year\": \"1900\"})\n",
    "    .astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"clean_date\" column of type timestamp.\n",
    "shark_clean[\"clean_date\"] = pd.to_datetime(\n",
    "    arg=shark_clean[[\"clean_year\", \"clean_month\", \"clean_day\"]]\n",
    "    .rename(columns={\"clean_day\": \"day\", \"clean_month\": \"month\", \"clean_year\": \"year\"}),\n",
    "    format=\"%Y-%m-%d\",\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "shark_clean[[\"clean_year\", \"clean_month\", \"clean_day\", \"clean_date\"]].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"fatal_(y/n)\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shark_clean[\"clean_fatal\"] = (\n",
    "    shark_clean[\"fatal_(y/n)\"]\n",
    "    .fillna(\"\")\n",
    "    .apply(\n",
    "        lambda fatal:\n",
    "        fatal.strip().lower() if fatal.strip().lower() == \"y\"\n",
    "        else fatal.strip().lower() if fatal.strip().lower() == \"n\"\n",
    "        else \"unknown\"\n",
    "    )\n",
    ")\n",
    "\n",
    "shark_clean[\"clean_fatal\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"injury\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shark_clean[\"injury\"].sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "injury_pattern = r\"no injury|fatal|bit|lacerat|sever|abrasion|drown|bruise|puncture|minor\"\n",
    "\n",
    "shark_clean[\"clean_injury\"] = shark_clean[\"injury\"].fillna(\"\").apply(\n",
    "    lambda injury:\n",
    "    re.findall(injury_pattern, injury.lower())[0] if len(re.findall(injury_pattern, injury.lower())) > 0\n",
    "    else \"unknown\"\n",
    ")\n",
    "shark_clean[[\"injury\",\"clean_injury\"]].sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename values to improve readability\n",
    "\n",
    "# Create dict with new names\n",
    "injury_name = {\n",
    "    \"bit\": \"bite\",\n",
    "    \"lacerat\": \"laceration\",\n",
    "    \"minor\": \"minor injury\",\n",
    "    \"sever\": \"severance\",\n",
    "    \"drown\": \"drowning\"\n",
    "}\n",
    "\n",
    "# Apply to \"clean_injury\" column\n",
    "shark_clean[\"clean_injury\"] = shark_clean[\"clean_injury\"].apply(\n",
    "    lambda injury: injury_name[injury]\n",
    "    if injury in injury_name.keys()\n",
    "    else injury\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build value_counts() of \"clean_injury\" column\n",
    "injury_value_counts = shark_clean[\"clean_injury\"].value_counts(normalize=True)\n",
    "injury_value_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"location\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_regex = r\"bay|dock|island|river|lake|reef|harbor|offshore|beach|port|cove|sea|ocean|pier|strait\"\n",
    "\n",
    "shark_clean[\"clean_location\"] = shark_clean[\"location\"].fillna(\"unknown\").apply(\n",
    "    lambda location:\n",
    "    re.findall(location_regex, location.lower())[0] if len(re.findall(location_regex, location.lower())) > 0\n",
    "    else location if location == \"unknown\"\n",
    "    else \"other\"\n",
    ")\n",
    "shark_clean[[\"location\",\"clean_location\"]].sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shark_clean[\"clean_location\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"sex\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shark_clean[\"clean_sex\"] = (\n",
    "    shark_clean[\"sex\"]\n",
    "    .fillna(\"\")\n",
    "    .apply(\n",
    "        lambda sex:\n",
    "        sex.lower() if sex == \"M\"\n",
    "        else sex.lower() if sex == \"F\"\n",
    "        else \"unknown\"\n",
    "           )\n",
    ")\n",
    "\n",
    "shark_clean[\"clean_sex\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"species\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file from the URL into a pandas DataFrame\n",
    "url = 'https://scipython.com/static/media/problems/P4.2/shark-species.txt'\n",
    "response = urllib.request.urlopen(url)\n",
    "lines = [l.decode('utf-8') for l in response.readlines()]\n",
    "species_df = pd.DataFrame([line.strip().split(':') for line in lines], columns=['Species', 'Common Name'])\n",
    "\n",
    "# Extract the common names and join them into a string separated by '|'\n",
    "common_names = species_df['Common Name'].str.strip().str.lower()\n",
    "common_names = common_names.fillna('')\n",
    "species_string = '|'.join(common_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \"shark\" from each species.\n",
    "species_string = re.sub(r\" ?shark\", \"\", species_string)\n",
    "\n",
    "# Replace any instance of \"[\" and \"]\" for \"\", as it will mes with regex syntax.\n",
    "species_string = re.sub(r\"[\\[\\]]\", \"\", species_string)\n",
    "\n",
    "# Replace any \" \" with \"\" in an effort to match more species with the DataFrame's extremely unstructured \"species\" column\n",
    "species_string = re.sub(\" \", \"\", species_string)\n",
    "\n",
    "# Replace any two or more \"|\" for only one \"|\" so as not to mess with regex syntax.\n",
    "species_string = re.sub(r\"\\|+\", \"|\", species_string)\n",
    "\n",
    "# Remove the first \"|\" before any word for the same reason as above.\n",
    "species_string = species_string[1:]\n",
    "\n",
    "# Add \"white\" species to the string, as it isn't there for some reason.\n",
    "species_string = species_string + \"|white\"\n",
    "\n",
    "# Print the species string\n",
    "print(species_string[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shark_clean[\"clean_species\"] = shark_clean[\"species\"].fillna(\"unknown\").apply(\n",
    "    lambda species: re.findall(species_string, species.replace(\" \", \"\").lower())[0]\n",
    "    if len(re.findall(species_string, species.replace(\" \", \"\").lower())) > 0\n",
    "    else \"unknown\"\n",
    ")\n",
    "\n",
    "shark_clean[[\"species\", \"clean_species\"]].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shark_clean[\"clean_species\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"time\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shark_clean[\"time\"].dropna().sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regex to standardize \"time\" column to only contain 2 values: \"a.m.\" and \"p.m.\"\n",
    "\n",
    "def standard_time(time_str):\n",
    "    re_am_pm = r\"a\\.m\\.|p\\.m\\.\"\n",
    "    re_pm_words = r\"evening|night|dusk|afternoon\"\n",
    "    re_am_words = r\"morning|dawn\"\n",
    "    re_time = r\"([0-9]{2})h([0-9]{2})\"\n",
    "\n",
    "    # Try first to find normal a.m. or p.m. match\n",
    "    try1 = re.findall(re_am_pm, time_str)\n",
    "    if try1:\n",
    "        return try1[0]\n",
    "\n",
    "    # If unsuccessful, move to p.m. words\n",
    "    try2 = re.findall(re_pm_words, time_str)\n",
    "    if try2:\n",
    "        return \"p.m.\"\n",
    "\n",
    "    # If unsuccessful yet again, move to a.m. words\n",
    "    try3 = re.findall(re_am_words, time_str)\n",
    "    if try3:\n",
    "        return \"a.m.\"\n",
    "\n",
    "    # If neither option is successful, move to time pattern\n",
    "    try4 = re.search(re_time, time_str)\n",
    "    if try4:\n",
    "        hours = int(try4.group(1))\n",
    "        if hours < 12:\n",
    "            return \"a.m.\"\n",
    "        elif hours >=12:\n",
    "            return \"p.m.\"\n",
    "\n",
    "    return \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply standard_time function to \"time\" column to create \"clean_time\" column\n",
    "shark_clean[\"clean_time\"] = shark_clean[\"time\"].fillna(\"\").map(standard_time)\n",
    "shark_clean[[\"time\", \"clean_time\"]].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shark_clean[\"clean_time\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"type\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shark_clean[\"type\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shark_clean[\"clean_type\"] = shark_clean[\"type\"].apply(\n",
    "    lambda atype: \"Boating\"\n",
    "    if atype == \"Boatomg\"\n",
    "    else atype\n",
    ")\n",
    "\n",
    "shark_clean[\"clean_type\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"For comparison reasons, shark_clean shape is {shark_clean.shape}\")\n",
    "categorical_cardinality = shark_clean.select_dtypes(include=\"object\").nunique().sort_values(ascending=False)\n",
    "categorical_cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 6))\n",
    "\n",
    "plt.bar(x=categorical_cardinality.index, height=categorical_cardinality, color=\"blue\")\n",
    "\n",
    "# Line-drawing and text annotation\n",
    "plt.axhline(y=shark_clean.shape[0], linestyle=\"--\")\n",
    "ax.annotate(\"Total row count\", xy=(22, shark_clean.shape[0] -400), fontsize=20)\n",
    "\n",
    "# x-labels\n",
    "ax.set_xlabel(\"Column Name\", fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=13)\n",
    "\n",
    "# y-labels\n",
    "ax.set_ylabel(\"Unique Values Count\", fontsize=14)\n",
    "plt.yticks(fontsize=13)\n",
    "\n",
    "# Title and legend\n",
    "ax.set_title(\"Unique Values - Categorical Columns\", fontsize=18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop high-cardinality categorical columns that won't be treated\n",
    "# (except \"case_number\", which will act as primary key).\n",
    "shark_clean = shark_clean.drop(columns=\n",
    "                               [\"pdf\", \"href_formula\", \"href\", \"name\", \"original_order\",\n",
    "                                \"case_number\", \"date\", \"investigator_or_source\", \"location\", \"injury\",\n",
    "                                \"species\", \"activity\", \"area\", \"time\", \"age\", \"type\", \"null_count\", \"year\", \"sex\", \"fatal_(y/n)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"For comparison reasons, shark_clean shape is {shark_clean.shape}\")\n",
    "new_categorical_cardinality = shark_clean.select_dtypes(include=\"object\").nunique().sort_values(ascending=False)\n",
    "new_categorical_cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 6))\n",
    "\n",
    "plt.bar(x=new_categorical_cardinality.index, height=new_categorical_cardinality, color=\"blue\")\n",
    "\n",
    "# x-labels\n",
    "ax.set_xlabel(\"Column Name\", fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=13)\n",
    "\n",
    "# y-labels\n",
    "ax.set_ylabel(\"Unique Values Count\", fontsize=14)\n",
    "plt.yticks(fontsize=13)\n",
    "\n",
    "# Title and legend\n",
    "ax.set_title(\"Unique Values - Categorical Columns\", fontsize=18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shark_clean[\"clean_date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 6))\n",
    "\n",
    "clean_null_count_cols = shark_clean.isnull().sum().sort_values()\n",
    "\n",
    "plt.bar(x=clean_null_count_cols.index, height=shark_clean.shape[0], color=\"blue\", label=\"Non-null\")\n",
    "plt.bar(x=clean_null_count_cols.index, height=clean_null_count_cols, bottom=shark_clean.shape[0]-clean_null_count_cols, color=\"red\", label=\"Null\")\n",
    "\n",
    "# Line-drawing and text annotation\n",
    "plt.axhline(y=shark_clean.shape[0]/2, linestyle=\"--\")\n",
    "ax.annotate(\"50%\", xy=(22, shark_clean.shape[0]/2 + 100), fontsize=25)\n",
    "\n",
    "# x-labels\n",
    "ax.set_xlabel(\"Column Name\", fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=13)\n",
    "\n",
    "# y-labels\n",
    "ax.set_ylabel(\"Null Value Count\", fontsize=14)\n",
    "plt.yticks(fontsize=13)\n",
    "\n",
    "# Title and legend\n",
    "ax.set_title(\"Null Values per Column - Cleaner Dataset\", fontsize=18)\n",
    "plt.legend(fontsize=16, loc='upper left', bbox_to_anchor=(1, 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Are provoked incidents more likely to be fatal?\n",
    "##### 2. Is an incident more likely to happen on specific locations depending on the time of day (a.m. or p.m.)?\n",
    "##### 3. According to the data at hand, which shark species is the most lethal? And the least lethal?\n",
    "##### 4. Is age a relevant factor when it comes to the fatality of an attack?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Are provoked incidents more likely to be fatal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mask to select only rows with \"Provoked\" or \"Unprovoked\" incidents\n",
    "# and \"n\" or \"y\" fatal incidents\n",
    "type_fatal_selection = (\n",
    "    shark_clean[\n",
    "        ((shark_clean[\"clean_type\"] == \"Provoked\") | (shark_clean[\"clean_type\"] == \"Unprovoked\"))\n",
    "        & ((shark_clean[\"clean_fatal\"] == \"y\") | (shark_clean[\"clean_fatal\"] == \"n\"))\n",
    "    ]\n",
    "    [[\"clean_type\", \"clean_fatal\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_fatal_gb = type_fatal_selection.groupby(by=[\"clean_type\", \"clean_fatal\"])\n",
    "tfgb_count = type_fatal_gb.agg(\n",
    "    count=(\"clean_type\", \"count\")\n",
    ").reset_index()\n",
    "tfgb_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a crosstab between the two columns\n",
    "tfgb_crosstab = pd.crosstab(type_fatal_selection[\"clean_fatal\"], type_fatal_selection[\"clean_type\"], normalize=\"index\")\n",
    "\n",
    "# create a heatmap using seaborn\n",
    "fig, ax = plt.subplots(figsize = (10, 8))\n",
    "sns.heatmap(tfgb_crosstab, cmap=\"Blues\", annot=True, fmt=\".2f\", ax=ax)\n",
    "\n",
    "# add labels and title\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style(\"white\")\n",
    "plt.xlabel(\"Type\")\n",
    "plt.ylabel(\"Fatal\")\n",
    "plt.title(\"Fatal vs Type Matrix Plot\")\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the counts for non-fatal unprovoked and fatal unprovoked incidents\n",
    "non_fatal_count_unprov = tfgb_count[\"count\"].iloc[2]\n",
    "fatal_count_unprov = tfgb_count[\"count\"].iloc[3]\n",
    "\n",
    "# Divide the counts\n",
    "ratio_unprov = round(fatal_count_unprov / non_fatal_count_unprov, 2)\n",
    "ratio_unprov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the counts for non-fatal provoked and fatal provoked incidents\n",
    "non_fatal_count_prov = tfgb_count[\"count\"].iloc[0]\n",
    "fatal_count_prov = tfgb_count[\"count\"].iloc[1]\n",
    "\n",
    "# Divide the counts\n",
    "ratio_prov = round(fatal_count_prov / non_fatal_count_prov, 2)\n",
    "ratio_prov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The proportion of provoked fatal incidents to provoked non-fatal incidents is {ratio_prov}, while the unprovoked incident ratio is {ratio_unprov}. According to this, unprovoked incidents are roughly {round(ratio_unprov/ratio_prov, 1)} times more fatal than provoked incidents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. On specific locations, is an incident more likely to happen depending on the time of day (a.m. or p.m.)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mask to select only rows with \"a.m.\" or \"p.m.\" incidents\n",
    "# and group shark_clean by \"location\" excluding rows with \"other\" and \"unknown\" location value\n",
    "loc_time_selection = (shark_clean[\n",
    "    (\n",
    "        (shark_clean[\"clean_location\"] != \"other\")\n",
    "        &\n",
    "        (shark_clean[\"clean_location\"] != \"unknown\")\n",
    "    )\n",
    "    &\n",
    "    (\n",
    "        (shark_clean[\"clean_time\"] == \"a.m.\")\n",
    "        |\n",
    "        (shark_clean[\"clean_time\"] == \"p.m.\")\n",
    "    )\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_gb = (\n",
    "    loc_time_selection\n",
    "    .groupby(by=[\"clean_location\", \"clean_time\"])\n",
    ")\n",
    "\n",
    "location_gb.agg(\n",
    "    count = (\"clean_location\", \"count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a crosstab between the two columns\n",
    "loc_time_crosstab = pd.crosstab(loc_time_selection[\"clean_location\"], loc_time_selection[\"clean_time\"], normalize=\"index\")\n",
    "\n",
    "# create a heatmap using seaborn\n",
    "fig, ax = plt.subplots(figsize = (10, 8))\n",
    "sns.heatmap(loc_time_crosstab, cmap=\"Blues\", annot=True, fmt=\".2f\", ax=ax)\n",
    "\n",
    "# add labels and title\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style(\"white\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Location\")\n",
    "plt.title(\"Location vs Time Matrix Plot\")\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a crosstab between the two columns\n",
    "loc_time_crosstab = pd.crosstab(loc_time_selection[\"clean_location\"], loc_time_selection[\"clean_time\"])\n",
    "\n",
    "# create a heatmap using seaborn\n",
    "fig, ax = plt.subplots(figsize = (10, 8))\n",
    "sns.heatmap(loc_time_crosstab, cmap=\"Blues\", annot=True, fmt=\".2f\", ax=ax)\n",
    "\n",
    "# add labels and title\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style(\"white\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Location\")\n",
    "plt.title(\"Location vs Time Matrix Plot\")\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. According to the data at hand, which shark species is the most lethal? And the least lethal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_fatal_selection = (shark_clean[\n",
    "    (shark_clean[\"clean_species\"] != \"unknown\")\n",
    "    &\n",
    "    (\n",
    "        (shark_clean[\"clean_fatal\"] == \"n\")\n",
    "        |\n",
    "        (shark_clean[\"clean_fatal\"] == \"y\")\n",
    "    )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_gb = species_fatal_selection.groupby(by=[\"clean_species\", \"clean_fatal\"])\n",
    "\n",
    "species_gb_count_agg = species_gb.agg(\n",
    "    count=(\"clean_species\", \"count\")\n",
    ").reset_index()\n",
    "species_gb_count_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_fatal_crosstab.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mplcursors\n",
    "\n",
    "# create a crosstab between the two and create \"prop\" and \"count\" column.\n",
    "species_fatal_crosstab = pd.crosstab(species_fatal_selection[\"clean_species\"], loc_time_selection[\"clean_fatal\"]).reset_index()\n",
    "species_fatal_crosstab[\"count\"] = species_fatal_crosstab[\"n\"] + species_fatal_crosstab[\"y\"]\n",
    "species_fatal_crosstab[\"prop\"] = species_fatal_crosstab[\"y\"] / species_fatal_crosstab[\"count\"]\n",
    "\n",
    "# create a heatmap using seaborn\n",
    "fig, ax = plt.subplots(figsize = (10, 8))\n",
    "scatter = sns.scatterplot(x=\"count\", y=\"prop\", hue=\"clean_species\", data=species_fatal_crosstab, ax=ax)\n",
    "\n",
    "# add labels and title\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style(\"white\")\n",
    "plt.xlabel(\"Incident Count\")\n",
    "plt.ylabel(\"Proportion of fatal to total\")\n",
    "plt.title(\"Incident Count vs Proportion of Fatality\")\n",
    "\n",
    "# remove legend\n",
    "ax.legend_.remove()\n",
    "\n",
    "# add hover effect\n",
    "cursor = mplcursors.cursor(scatter)\n",
    "cursor.connect(\"add\", lambda sel: sel.annotation.set_text(f\"{sel.artist.get_label()}: Count={sel.target[0]:.0f}, Proportion={sel.target[1]:.2f}\"))\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a crosstab between the two and create \"prop\" and \"count\" column.\n",
    "species_fatal_crosstab = pd.crosstab(species_fatal_selection[\"clean_species\"], loc_time_selection[\"clean_fatal\"]).reset_index()\n",
    "species_fatal_crosstab[\"count\"] = species_fatal_crosstab[\"n\"] + species_fatal_crosstab[\"y\"]\n",
    "species_fatal_crosstab[\"prop\"] = species_fatal_crosstab[\"y\"] / species_fatal_crosstab[\"count\"]\n",
    "\n",
    "# create hover effect\n",
    "sns.relplot(x=\"count\", y=\"prop\", hue=\"clean_species\", size=\"count\", sizes=(50, 500), data=species_fatal_crosstab, legend=False)\n",
    "\n",
    "# add labels and title\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style(\"white\")\n",
    "plt.xlabel(\"Incident Count\")\n",
    "plt.ylabel(\"Proportion of fatal to total\")\n",
    "plt.title(\"Incident Count vs Proportion of Fatality\")\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
